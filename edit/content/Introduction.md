
## A Brief History

Data science has become a wildly popular job over the past ten years and it is commonly believed that the field is new. However, the field dates back to the early 1960s when the US Air Force established a Data Sciences Lab that applied artificial intelligence to the vast amounts processing and analysis of signal data that were being generated by satellites, radar, and other sources. In the following decades the field developed into a general practice associated with the management and processing of data to support scientific research, especially in fields that employed large data sets to study complex systems such as climate. More recently the field has become attached to the data-driven economy and the work of working with data across a wide array of domains, from social media to medicine. Because of its popularity in industry, many colleges and universities offer degrees in data science.

## Introduction

In 2008, the year of Google's tenth anniversary, the primary meaning of the term "data science" (and the related term "data scientist") changed. From at least the early 1990s up to this time the term referred to an established if obscure field dedicated to the processing and management of scientific data. This field was represented by journals such as CODATA's *Data Science Journal* (founded in 2002) and addressed in high-level policy reports from the NSF in the US and JISC in the UK (Simberloff et al. 2005; Swan and Brown 2008). After 2008, resulting from the usage of "data scientist" by dominant internet companies like LinkedIn and Facebook, and promoted by influential bloggers like Nathan Yau of *FlowingData*, the term shifted meaning to a wildly popular activity within the data-driven corporation (Davenport and Patil 2012; Hammerbacher 2009; Yau 2009b). The newer usage referred to an advanced form of business intelligence, broadly defined, an eclectic mixture of statistics, data mining, artificial intelligence, computer science, and a host of recently developed data processing technologies designed to store and process so-called big data&mdash;data generated by social media, scientific instruments, transactional machinery, embedded sensors, and a variety of other sources.[^1]

Earlier, in the 1990s and early 2000s, the term was appropriated by a handful of academic statisticians in Japan and the U.S. seeking to expand and rebrand their field in response to the rapid growth and overshadowing effects of computational statistics and data mining (Ohsumi 1994; Wu 1997; Hayashi 1998; Cleveland 2001). However, this usage never caught on.[^2]

Since 2009, the growth of the term's new usage and its cluster of
associated activities has been exponential. This growth has been
associated with a high demand for data scientists, a story that
continues to be covered by the news media. The response by institutions
of higher education to train data scientists to meet industry demand has
been rapid and pronounced. Since the shift in meaning, hundreds of
master's degree programs in data science and closely related fields have
been established in the United States, both residential and online. More
recently, a handful of doctoral programs and schools of data science
have emerged, along with undergraduate offerings to meet increasing
student demand.

One effect of these developments has been to stimulate a preferential
attachment process within the network of disciplines that constitute the
academy: as a field representing the "sexiest job of the 21^st^
century," attracting students, gifts, and internal resources, many
adjacent disciplines&mdash;from systems engineering and computer science to
statistics and a variety of quantitative and computational
sciences&mdash;have sought to associate themselves with the field. Indeed,
because data science has had no history in the academy, these contiguous
fields have provided the courses and faculty out of which the majority
of data science programs have been built. The result is that data
science has become a complex and internally competitive patchwork of
industrial and academic interests and perspectives, reflecting the
broader engagement of society with data and its analysis beyond the
concept of data science inherited from industry.

From this confluence of sources a variety of definitions of data science
have emerged. One common view within the academy is that data science is
a buzzword that just means statistics (or data analysis) rebranded in
the era of widespread computational technology. For others, the field
represents a new paradigm of science based on data abundance and *in
silico* experimentation. Some recall the older meaning, as the science
of processing and managing data to support scientific research. Outside
of academia, definitions may range, as mentioned, from a form of
business intelligence for the data-driven firm to the application of
machine learning and artificial intelligence to big data to solve real
world problems. To complicate matters further, the job descriptions of
data scientists overlap significantly with those of other roles, aside
from statistician and data analyst, ranging from business analyst to
machine learning engineer. And then there are fields like critical data
studies which bring their own understandings of data science in their
efforts to describe its social consequences and ethical obligations.

This variety is not necessarily a bad thing&mdash;it reflects the vibrant
fertility in the field that has contributed to its success. But when it
comes to building academic programs to meet the great demand for data
scientists, clear definitions matter. The categories, values, and
relationships they make explicit determine everything from the design of
curricula to the selection of faculty to the allocation of resources for
research. Academic programs have an obligation to arrive at a coherent
and compelling understanding of the field that will situate it among its
fellow disciplines on campus and remain relevant to the extra-academic
organizations where data scientists work. Such an understanding should
be developed from a clear understanding of the origin of data science
and why it has emerged as such a powerfully attractive rubric under
which people and resources are being marshalled.

So, how do we define the field so that we preserve its richness and
utility in a dynamic world while having a clear idea of its central
concerns and ideas? To answer this, we need to do more than produce a
list of all the things that data scientists do in the hopes that some
order might emerge from it. Nor are high-level constructs like
simplified Venn diagrams helpful here. In both cases, important details
and questions of meaning and priority are left to interpretation. What
we need is an understanding of why it is that such a diverse collection
of definitions would appear together in the first place. What principle
or theme, if any, brings them together?

## Making Sense of the Field

Ever since Nathan Yau corrected Hal Varian's usage of "statistician" to
"data scientist" in reference to Varian's famous quip about the former
being the "sexy job in the next ten years," there has been no shortage
of attempts to make sense of the new field (McKinsey & Company 2009; Yau
2009b).[^3] These generally come from either industry or academia. Among
the former are early responses from the blogosphere (Yau 2009b; 2009c;
2009a; Conway 2010; Mason and Wiggins 2010) followed by those of
industry thought leaders, such as O'Reilly (Loukides 2011), McKinsey
(Manyika et al. 2011), Harvard Business Review (Davenport and Patil
2012), Gartner (Laney and Kart 2012), and Booz Allen Hamilton (Herman et
al. 2013). Among the academic responses are those from statisticians,
the most popular being Donoho's trenchant "50 Years of Data Science"
(Donoho 2017 \[2015\]).

For the most part, the responses from industry are descriptive and
approving while the academic ones are critical and prescriptive. Those
in industry have sought to make sense of data science as a new field to
those wishing to hire data scientists, emphasizing the unique mixture of
roles it involves, from data wrangling and exploration to software
engineering and the visual communication of results. In contrast,
academic statisticians have reacted with surprise and concern at the
glaring disconnect between their field and data science (Davidian 2013;
Rodriguez 2012; Rodriguez, Davidian, and Schenker 2013) and have
exhorted their colleagues to take ownership of the new field (Yu 2014).
Indeed, Donoho's essay is a manifesto for doing so. He charts out the
territory of "Greater Data Science" (GDS)&mdash;a play on Chambers' earlier
plea for a "greater statistics" that would be "based on an inclusive
concept of learning from data"&mdash;and places statistics at its center
(Chambers 1993: 1). In addition, he locates GDS in a genealogy that
begins with data analysis&mdash;a practice envisioned in the 1960s by his
mentor at Princeton, the legendary mathematician John Tukey, who serves
as the founding ancestor in this legitimation narrative. GDS is thus
defined as "a superset of the fields of statistics and machine learning,
which adds some technology for 'scaling up' to 'big data'" (Donoho 2017
\[2015\]: 745).

Naturally, for the purposes of academic program building the academic
contributions have been more influential. Donoho's essay in particular
is often cited as authoritative (e.g. Cao 2017). It provides a
compelling narrative for the origin of data science (as we know it
today) and a comprehensive taxonomy of activities that might constitute
the field. It is tempting to accept this work as a foundation for
building out academic programs.

However, Donoho's framework has been criticized for downplaying the
contribution of computational technology to data science. In the
response section of Donoho's essay, Chris Wiggins, Chief Data Scientist
of the *New York Times* and a professor at Columbia, senses this and
asserts that data science is a form of engineering that will be defined
by its practitioners, not by academics trying to turn it into a (pure)
science. In his response, Sean Owen, then Director of Data Science at
Cloudera, argues that Donoho's history excludes the significant
contributions of data engineering (Donoho 2017: 764). Owen's criticism
is corroborated by the fact that Donoho's history also elides the
contributions of data collection and earlier iterations of the field of
data science, especially as it has been practiced in the U.K. Elsewhere,
Bryan and Wickham point out that, like many statisticians, Donoho
mistakenly relegates computational work to superficial status while also
missing "the full process of analysis" in which statistics "is but one
small part" (Bryan and Wickham 2017). In his defense, Donoho
acknowledges his bias, but justifies it by noting that although
technological know-how is important, technologies are transitory and
prone to rapid obsolescence, and therefore "the academic community can
best focus on teaching broad principles&mdash;'science' rather than
'engineering'" (Donoho 2017: 765). *Scientia longa, brevis ars.*

Donoho's marginalization of computational technology in his definition
of data science is consistent with a larger conflict between what
Breiman famously called "two cultures" in the field of statistics,
adapting the expression C.P. Snow used to characterize the split between
the sciences and the humanities (Breiman 2001; Snow 2013 \[1959\]). In
brief, one culture seeks to represent causality&mdash;the black box of
nature that generates the empirical data with which statistics
begins&mdash;by means of probabilistic or stochastic data models. The
parameters, random variables, and relationships that compose these
models are imagined to correspond to things in the world, at least in
principle. Data are used to estimate the parameters of these models.
This is the "data modeling culture," associated with traditional
statistics and data analysis. Breiman guessed this culture comprised 98%
of all statisticians. The other culture bypasses attempts to directly
model the contents of the black box and instead focuses on accounting
for the data by means of goal-oriented algorithms, regardless of the
correspondence of these to the world. This is the "algorithmic modeling
culture," associated with computer science, machine learning, and data
mining. Breiman described the growth of this culture as "rapid"
(beginning circa 1985) and characterized its results as "startling" and
suggested that statisticians ought to incorporate these methods into
their field (Breiman 2001: 200). As is evident from the essay's tepid
comments and from Donohoe's account of its weak reception by academic
statisticians, Breiman's advice went unheeded.

At the heart of this conflict is the division between data analysis,
championed by Donoho, and data mining (sometimes called "knowledge
discovery in databases"), a field that emerged in the 1980s and '90s
with the rise of available computational technology and the growth of
commodity databases, later spurred by the Web and social media. With
these resources in place&mdash;effectively forming a new infrastructure for
knowledge work&mdash;it became possible for the computationally-minded to
pursue a purely data-centric approach to statistical problems that in
many respects violates the principles and sensibilities of traditional
statistics. It is no coincidence that Breiman's essay appeared at about
the time some academic statisticians sought to rebrand their field as
data science in an attempt to integrate the gains of computational
technology while purging it of the methodological sins of data mining.

Contrary to Donoho's narrative, data mining is as much of an antecedent
to today's form of data science as data analysis, if not more so. A
direct connection exists between the data mining practices developed and
codified in the 1990s and what Hammerbacher would later call data
science, even if he may not have been aware of the connection. The
general goal in both cases was to retrieve information from databases
that had grown to the point of exceeding their ability to be queried for
the work of business intelligence. Concretely, Hammerbacher's work at
Facebook continued the kind of work pioneered by Amit Patel a few years
earlier at Google, who applied data mining methods to the logs the
search giant had been accumulating and revealed their predictive power
(Levy and Ganser 2012: 44--46). In both cases, the data were not
acquired for the purpose of supporting predictive or inferential
analyses, yet the data mining approach found enough value in them to
build companies that would not only become hugely profitable but, in the
case of Google, a model for science itself.

Regarding the connection between science and data mining that Google
shocked the world into recognizing, consider the impact data mining has
had on how the scientific method has been reimagined in the era of
pervasive computing. Gray's framing of e-science as a fourth paradigm is
essentially data mining and computational thinking applied explicitly
and broadly to science, both in terms of specific methods and general
spirit (Gray 2007; Hey, Tansley, and Tolle 2009). The fourth paradigm
argues for the centrality of database theory and practice to the
scientific method, to bring analysis closer to where increasingly large
and complex sets of data physically live, as well for the application of
the core data mining methods of pattern and rule discovery beyond
traditional hypothesis testing. Essential to this view is that database
technologies are not merely add-ons to a process that can be imagined
without them; the scientist now inhabits an infrastructure that
constitutes her experience. Science now trades in forms of knowledge
that do not exist without the computational technologies that generate
them. What Gray says about astronomy is a parable for all science:

People now do not actually look through telescopes. Instead, they are
"looking" through large-scale, complex instruments which relay data to
datacenters, and only then do they look at the information on their
computers (Hey, Tansley, and Tolle 2009: xix).

In other words, databases and the workflows built around them constitute
the symbolic form of science (Manovich 1999); they are the medium of
knowledge, not its vehicle. And this is the perspective of data mining.

A comprehensive definition of data science, then, would complement
Donoho's with an authentic representation of the contribution of
computational technologies and practices to the field. It would
encompass data analysis and data mining and would embrace the epistemic
tensions that exist between these ancestral fields, such as those
between mathematical statistics and computational thinking, and between
mathematical models and those that emerge from data.